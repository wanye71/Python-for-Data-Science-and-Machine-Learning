{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae3d7aa-c7e6-4926-b05a-91f51341f69a",
   "metadata": {},
   "source": [
    "# Asynchronous scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "89b690a4-945b-41ea-b438-1bb1c84e7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install aiohttp asyncio\n",
    "# !pip install nest-asyncio\n",
    "# !pip install aiofiles\n",
    "# !pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7359846f-691e-4ec2-a19b-5a3992c99dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the realm of asynchronous, where tasks take flight,\n",
    "# 'aiohttp' is summoned, for HTTP requests right.\n",
    "\n",
    "# aiohttp is a library used for making asynchronous HTTP requests, allowing\n",
    "# for efficient handling of multiple concurrent requests.\n",
    "import aiohttp\n",
    "\n",
    "# Amidst the async dance, where coroutines align,\n",
    "# 'asyncio' joins, a framework so fine.\n",
    "\n",
    "# asyncio is a library used for managing asynchronous tasks in Python,\n",
    "# providing a framework for writing asynchronous code using coroutines.\n",
    "import asyncio\n",
    "\n",
    "# In the async world, where files are prime,\n",
    "# 'aiofiles' steps forth, in an asynchronous rhyme.\n",
    "\n",
    "# aiofiles is a library used for asynchronous file operations in Python,\n",
    "# enabling efficient reading and writing of files in an asynchronous manner.\n",
    "import aiofiles\n",
    "\n",
    "# Amidst the data's realm, where rows and columns shine,\n",
    "# 'csv' is called, a reader so fine.\n",
    "\n",
    "# csv is a module in Python's standard library used for reading and writing\n",
    "# CSV files, providing functionality to handle comma-separated values data.\n",
    "import csv\n",
    "\n",
    "# In the realm of text, where patterns intertwine,\n",
    "# 're' stands strong, for regex design.\n",
    "\n",
    "# re is a module in Python's standard library used for working with regular\n",
    "# expressions, providing powerful pattern matching capabilities.\n",
    "import re\n",
    "\n",
    "# In the soup of HTML, where tags come alive,\n",
    "# 'BeautifulSoup' reigns, to parse and derive.\n",
    "\n",
    "# BeautifulSoup is a Python library used for parsing HTML and XML documents,\n",
    "# providing tools for extracting data from HTML/XML files and navigating\n",
    "# through their structure.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0fdc4e0f-2bb0-43fe-b6ec-6e1d75e6d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "677590ea-61e9-4ad2-9011-959cf28a0838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the digital realm, where tasks take flight,\n",
    "# 'fetch' is defined, to fetch data right.\n",
    "# With HTTP requests, it ventures forth,\n",
    "# To bring back data from the internet's vast north.\n",
    "\n",
    "# fetch is an asynchronous function used to make HTTP GET requests\n",
    "# using the provided aiohttp session and URL. It returns the text content\n",
    "# of the response if successful, otherwise prints an error message.\n",
    "async def fetch(session, url):\n",
    "    try:\n",
    "        # The 'try' block allows us to execute code that may potentially\n",
    "        # raise an exception. If an exception occurs within the 'try' block,\n",
    "        # execution immediately jumps to the 'except' block.\n",
    "        # The 'async with' statement is used to asynchronously acquire\n",
    "        # a resource (in this case, an HTTP response from the provided URL)\n",
    "        # and automatically release it when the block of code is exited.\n",
    "        async with session.get(url) as response:\n",
    "            # Inside the 'async with' block, the response object is available\n",
    "            # for further processing. Here, we await the response text,\n",
    "            # which asynchronously reads the response content.\n",
    "            return await response.text()\n",
    "    except Exception as e:\n",
    "        # The 'except' block is used to handle exceptions that occur\n",
    "        # within the 'try' block. Here, we catch all exceptions (denoted\n",
    "        # by 'Exception') and print an error message with the exception details.\n",
    "        print(\"Fetch error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c94d5e4f-1610-4133-99c6-c1ebe4a7fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amidst the web's labyrinth, where links entwine,\n",
    "# 'scrape_and_save_links' is defined, a function so fine.\n",
    "# With HTML parsed, and a soup to savor,\n",
    "# It plucks out links with a digital flavor.\n",
    "\n",
    "# scrape_and_save_links is an asynchronous function used to parse\n",
    "# HTML text using BeautifulSoup and extract all links that start with\n",
    "# 'http' from the provided text. It returns a list of extracted links.\n",
    "async def scrape_and_save_links(url, text):\n",
    "    # Create a BeautifulSoup object to parse the HTML text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    \n",
    "    # Extract all links from the parsed HTML that start with 'http'\n",
    "    links = [link.get('href') for link in soup.find_all('a', href=True) if link['href'].startswith('http')]\n",
    "    \n",
    "    # Return the extracted links\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cea4a832-121f-4067-a15d-8b8676f8b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the async realm, where tasks unite,\n",
    "# 'scrape' is defined, to scrape data's light.\n",
    "# With URLs to traverse, and files to scribe,\n",
    "# It gathers links, in an async vibe.\n",
    "\n",
    "# scrape is an asynchronous function used to scrape data from multiple URLs\n",
    "# using aiohttp and aiofiles libraries. It writes the extracted links to a CSV file.\n",
    "async def scrape(urls, filename):\n",
    "    # Create an aiohttp ClientSession to make asynchronous HTTP requests\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Open the file in write mode using aiofiles, with UTF-8 encoding\n",
    "        async with aiofiles.open(filename, 'w', encoding='utf-8') as file:\n",
    "            # Create a CSV writer object to write to the file\n",
    "            writer = csv.writer(file)\n",
    "            # Write the header row to the CSV file\n",
    "            headers = [\"Analytics\", \"Python\", \"LinkedIn\", \"Pandas\"]\n",
    "            await writer.writerow(headers)\n",
    "            \n",
    "            # Asynchronously gather the results of scraping links from each URL\n",
    "            results = await asyncio.gather(*[scrape_and_save_links(url, await fetch(session, url)) for url in urls])\n",
    "            \n",
    "            # Find the maximum number of links among all results\n",
    "            max_links = max(len(links) for links in results)\n",
    "            \n",
    "            # Iterate over the range of maximum links\n",
    "            for i in range(max_links):\n",
    "                # Create a row for the CSV file with links from each result\n",
    "                row = [links[i] if i < len(links) else \"\" for links in results]\n",
    "                # Write the row to the CSV file\n",
    "                await writer.writerow(row)\n",
    "\n",
    "        # After writing to the file, open it again to read and print its contents\n",
    "        \n",
    "        # async with aiofiles.open(filename, 'r', encoding='utf-8') as file:\n",
    "        #     # Read the entire file content\n",
    "        #     content = await file.read()\n",
    "        #     # Print the content\n",
    "        #     print(content)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0f840f0b-c0cc-4827-85ea-f978208ee92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the async realm, where tasks convene,\n",
    "# 'main' is defined, to orchestrate the scene.\n",
    "# With URLs to explore, and files to create,\n",
    "# It calls 'scrape', in a state so great.\n",
    "\n",
    "# main is an asynchronous function that serves as the entry point\n",
    "# for the program. It orchestrates the scraping process by defining\n",
    "# URLs to scrape and the filename for the CSV output.\n",
    "async def main():\n",
    "    # Define a list of URLs to scrape\n",
    "    urls = ['https://analytics.usa.gov', 'https://python.org', 'https://linkedin.com', 'https://pandas.pydata.org/']\n",
    "    \n",
    "    # Define the filename for the CSV output\n",
    "    filename = 'myLinks.csv'\n",
    "    \n",
    "    # Asynchronously call the scrape function to scrape data from the URLs\n",
    "    await scrape(urls, filename)\n",
    "    return pd.read_csv(filename).fillna('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fe1f8d26-3e57-4412-8392-a10a028c656e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analytics</th>\n",
       "      <th>Python</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>Pandas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://open.gsa.gov/api/dap/</td>\n",
       "      <td>https://www.python.org/psf/</td>\n",
       "      <td>https://www.linkedin.com/pulse/topics/home/?tr...</td>\n",
       "      <td>https://stackoverflow.com/questions/tagged/pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://analytics.usa.gov/data/live/top-10000-...</td>\n",
       "      <td>https://docs.python.org</td>\n",
       "      <td>https://www.linkedin.com/pub/dir/+/+?trk=guest...</td>\n",
       "      <td>https://www.python.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://digital.gov/guides/dap/</td>\n",
       "      <td>https://pypi.org/</td>\n",
       "      <td>https://www.linkedin.com/learning/search?trk=g...</td>\n",
       "      <td>https://stackoverflow.com/questions/tagged/pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://open.gsa.gov/api/dap/</td>\n",
       "      <td>https://psfmember.org/civicrm/contribute/trans...</td>\n",
       "      <td>https://www.linkedin.com/jobs/search?trk=guest...</td>\n",
       "      <td>https://numfocus.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/18F/analytics.usa.gov/issues</td>\n",
       "      <td>https://www.linkedin.com/company/python-softwa...</td>\n",
       "      <td>https://www.linkedin.com/signup?trk=guest_home...</td>\n",
       "      <td>https://www.twosigma.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/legal/cookie-policy?t...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/legal/copyright-polic...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://brand.linkedin.com/policies?trk=homepa...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/psettings/guest-contr...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.linkedin.com/legal/professional-co...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Analytics  \\\n",
       "0                        https://open.gsa.gov/api/dap/   \n",
       "1    https://analytics.usa.gov/data/live/top-10000-...   \n",
       "2                      https://digital.gov/guides/dap/   \n",
       "3                        https://open.gsa.gov/api/dap/   \n",
       "4      https://github.com/18F/analytics.usa.gov/issues   \n",
       "..                                                 ...   \n",
       "137                                                      \n",
       "138                                                      \n",
       "139                                                      \n",
       "140                                                      \n",
       "141                                                      \n",
       "\n",
       "                                                Python  \\\n",
       "0                          https://www.python.org/psf/   \n",
       "1                              https://docs.python.org   \n",
       "2                                    https://pypi.org/   \n",
       "3    https://psfmember.org/civicrm/contribute/trans...   \n",
       "4    https://www.linkedin.com/company/python-softwa...   \n",
       "..                                                 ...   \n",
       "137                                                      \n",
       "138                                                      \n",
       "139                                                      \n",
       "140                                                      \n",
       "141                                                      \n",
       "\n",
       "                                              LinkedIn  \\\n",
       "0    https://www.linkedin.com/pulse/topics/home/?tr...   \n",
       "1    https://www.linkedin.com/pub/dir/+/+?trk=guest...   \n",
       "2    https://www.linkedin.com/learning/search?trk=g...   \n",
       "3    https://www.linkedin.com/jobs/search?trk=guest...   \n",
       "4    https://www.linkedin.com/signup?trk=guest_home...   \n",
       "..                                                 ...   \n",
       "137  https://www.linkedin.com/legal/cookie-policy?t...   \n",
       "138  https://www.linkedin.com/legal/copyright-polic...   \n",
       "139  https://brand.linkedin.com/policies?trk=homepa...   \n",
       "140  https://www.linkedin.com/psettings/guest-contr...   \n",
       "141  https://www.linkedin.com/legal/professional-co...   \n",
       "\n",
       "                                                Pandas  \n",
       "0    https://stackoverflow.com/questions/tagged/pandas  \n",
       "1                               https://www.python.org  \n",
       "2    https://stackoverflow.com/questions/tagged/pandas  \n",
       "3                                https://numfocus.org/  \n",
       "4                            https://www.twosigma.com/  \n",
       "..                                                 ...  \n",
       "137                                                     \n",
       "138                                                     \n",
       "139                                                     \n",
       "140                                                     \n",
       "141                                                     \n",
       "\n",
       "[142 rows x 4 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the async realm, where tasks take flight,\n",
    "# 'main' is run, with all its might.\n",
    "# It orchestrates the show, with a masterful hand,\n",
    "# Bringing async tasks to a grand, final stand.\n",
    "\n",
    "# Use asyncio.run() to run the 'main' coroutine,\n",
    "# starting the asynchronous program execution.\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d2dff-b513-4347-b75c-1eec56f57668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
