# Python for Data Science and Machine Learning

## Table of Contents

1. [Filtering and selecting](#filtering-and-selecting)
2. [Treating missing values](#treating-missing-values)
3. [Removing duplicates](#removing-duplicates)
4. [Concatenating and transforming](#concatenating-and-transforming)
5. [Grouping and aggregation](#grouping-and-aggregation)

### Filtering and selecting
```python
# Aight, let's get this party started with some heavy hitters!

# First up, we're summoning the 'np' library into the mix. This bad boy is like the Swiss Army knife of numerical computing!
# It's like having a superhero by our side, ready to crunch numbers and tackle any math challenge.

import numpy as np  # Get ready to crunch numbers like a boss!

# Next on the lineup, we're bringing in the 'pd' library. This powerhouse is the backbone of our data adventures!
# It's like having a master chef in the kitchen, ready to whip up delicious datasets and serve them with style.

import pandas as pd  # Get ready to dive deep into the world of data with pandas!

# And last but not least, we're summoning the 'DataFrame' class from the 'pandas' library.
# It's like having the VIP pass to the data party, giving us access to all the exclusive features and functionalities.

from pandas import DataFrame  # Get ready to create some epic DataFrames!


# Now, let's use the 'DataFrame' class to create our DataFrame.
# We're passing the sequence of numbers generated by 'np.arange' to the DataFrame constructor.
# We're also specifying the shape of the DataFrame as 10 rows and 3 columns using the 'reshape' function.
# It's like sculpting our data masterpiece, molding it into the perfect shape.

numbers_df = DataFrame(
    np.arange(0, 90, 3).reshape(10, 3),  # Generating a sequence of numbers from 0 to 90 (exclusive) with a step size of 3, reshaping it into a 10x3 array
    index=['row 1', 'row 2', 'row 3', 'row 4', 'row 5', 'row 6', 'row 7', 'row 8', 'row 9', 'row 10'],  # Assigning index labels for each row
    columns=['column 1', 'column 2', 'column 3']  # Assigning column labels for each column
)

numbers_df  # Returning the DataFrame

# Get ready to feast your eyes on the data masterpiece we've just created!


# Now, let's break down this code snippet and understand what each part does:

# numbers_df: This is our DataFrame object containing the numerical data.

# .iloc: This is a method of the DataFrame object used for integer-location based indexing.
# It allows us to access specific rows and columns using integer indices.

# [0, 1]: Within the .iloc method, we're specifying the row and column indices we want to access.
# In this case, 0 refers to the first row (because indexing starts from 0) and 1 refers to the second column.

# So, numbers_df.iloc[0, 1] retrieves the value at the intersection of the first row and second column in the DataFrame numbers_df.

# In simpler terms, it's like pinpointing a specific cell in a table (DataFrame) by its row and column numbers,
# and fetching the value stored in that cell.

# Let's execute the code and see what value it retrieves.

numbers_df.iloc[0, 1]


# Alright, let's break it down step by step:

# We're assigning the value 20 to the cell located at the intersection of the first row and second column in the DataFrame 'numbers_df'.
# The .iloc method is used for integer-location based indexing, and [0, 1] specifies the row and column indices.
# So, we're modifying the value at row 1, column 2 (because indexing starts from 0).

numbers_df.iloc[0, 1] = 20

# Now, let's print the updated DataFrame 'numbers_df' to see the changes made.
# We're displaying the DataFrame after the value assignment to visualize the updated data.

numbers_df


# Yo, check it out! We're about to flex some data skills:

# We're using fancy indexing with the .iloc method to select specific rows and columns from 'numbers_df'.
# This line of code is like cherry-pickin' the freshest data points from the block.
# We're rollin' with rows 1, 3, and 5, and columns 0 and 2.

numbers_df.iloc[[1, 3, 5], [0, 2]]


# Alright, let's break it down street style:

# We're cookin' up a boolean mask by layin' down a condition on the DataFrame 'numbers_df'.
# In this case, we're checkin' if each value in 'numbers_df' is straight-up greater than 30.
# The result is a DataFrame of boolean values, where 'True' means that the corresponding value in 'numbers_df' is higher than 30,
# and 'False' means it ain't.

mask = numbers_df > 30

# Now, let's peep the boolean mask to see what's poppin'.
# The mask shows us which values in 'numbers_df' keep it real by meetin' the condition (higher than 30) and which ones ain't cuttin' it.

mask


# Alright, check it out fam, we're about to drop some mad skills:

# We're usin' the boolean mask we cooked up earlier to filter out the values in 'numbers_df' that match the condition.
# This line of code is like siftin' through the data hood and pickin' out only the values that meet the criteria.
# We're only keepin' it real with the values that passed the test, where the corresponding cell in 'mask' is 'True'.

numbers_df[mask]


# Yo, check it fam! We're about to flip the script on these numbers:

# We're using a conditional statement to create a boolean mask where values in 'numbers_df' greater than 30 are flagged.
# Then, we're setting all these high-rolling numbers to 0, like wiping the slate clean.
# It's like saying, "Nah, we ain't playin' that game no more!"

numbers_df[numbers_df > 30] = 0

# Now, let's peep the updated 'numbers_df' after droppin' those high flyers down to zero.
# It's a whole new vibe, like starting fresh on a brand new day!

numbers_df


# Yo, peep this! We're about to slice and dice some data:

# We're using the .iloc method to select a range of rows from index 2 to 5 (exclusive) and columns from index 1 to 2 (exclusive).
# It's like carvin' out a fresh slice of data pie, grabbin' only the juiciest bits from rows 3 to 6 and columns 2 to 3.

numbers_df.iloc[2:6, 1:3]
```

### Treating missing values
```python
# Yo, we're gearing up for some serious data hustlin':

# First up, we're summoning the almighty numpy library, ready to crunch numbers like a boss.
# It's like having a math wizard by our side, ready to drop some serious calculations.

# Get ready to crunch numbers like a boss!
import numpy as np  

# Next in line, we're bringing in the pandas library to the mix.
# This powerhouse is the backbone of our data adventures, letting us tame and wrangle our datasets with ease.
# It's like having a data guru on standby, guiding us through the data jungle.

# Get ready to dive deep into the world of data with pandas!
import pandas as pd  

# And last but not least, we're cherry-picking the DataFrame class from the pandas library.
# It's like snagging the VIP pass to the data party, granting us access to all the exclusive features and functionalities.
# We're ready to craft some epic DataFrames!

# Get ready to create some epic DataFrames!
from pandas import DataFrame  

### Filling missing values using fillna(), replace() and interpolate()
# Aight, let's cook up some fresh data, fam:

# We're crafting a dictionary called 'data' containing the dope details of our crew.
# Each key represents a feature: 'names', 'age', 'gender', and 'rank'.
# It's like putting together a crew roster for our data squad, complete with their stats.

data = {'names': ['Steve', 'John', 'Richard', 'Sarah', 'Randy', 'Michael', 'Julie'],
        'age': [20, 22, 20, 21, 24, 23, 22],
        'gender': ['Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female'],
        'rank': [2, 1, 4, 5, 3, 7, 6]}

# Now, let's assemble this crew into a DataFrame called 'ranking_df'.
# It's like putting together our dream team lineup, ready to conquer the data world!

ranking_df = DataFrame(data)

# Let's peep the 'ranking_df' to see our crew in action.
# Get ready to meet the squad and see who's bringin' the heat!

ranking_df

# Yo, we're about to shake things up with some missing values:

# We're using .iloc to target rows 2 to 4 (inclusive) and column 1, and setting those values to NaN.
# It's like creating a little mystery in our data, leaving those spots empty for now.

ranking_df.iloc[2:5, 1] = np.nan

# Now, we're targeting rows 3 to 5 (inclusive) and column 3, and dropping some NaN bombs in there.
# It's like adding another layer of intrigue, making our data even more mysterious.

ranking_df.iloc[3:6, 3] = np.nan

# Oh snap! We're taking it up a notch and dropping an entire row of NaN values.
# It's like saying, "Peace out, row 4! You're on vacation now."

ranking_df.iloc[3, :] = np.nan

# Now, let's peep the updated 'ranking_df' to see the effects of our data shenanigans.
# Get ready for some missing values, adding a bit of spice to our crew lineup!

ranking_df

# Yo, check it! We're about to uncover some hidden gems:

# We're using the .isnull() method to detect missing values (NaN) in the 'ranking_df'.
# It's like shining a spotlight on our data, exposing those empty spots we left behind.

ranking_df.isnull()

# Yo, fam! We're flipping the script and checking out the real deal:

# We're unleashing the .notnull() method on the 'ranking_df' to spot all the places where we ain't got no missing values.
# It's like shining a spotlight on the data and highlighting all the spots that are filled with real, solid information.

ranking_df.notnull()

# Yo, check it! We're about to uncover some hidden gems:

# We're using the pd.isnull() function to create a boolean series called 'bool_series'.
# This series will have True where the 'age' column in 'ranking_df' has missing values (NaN), and False otherwise.
# It's like shining a flashlight on the 'age' column and flagging all the spots where we're missing data.

bool_series = pd.isnull(ranking_df['age'])

# Now, we're filtering 'ranking_df' using 'bool_series' to display only the rows where the 'age' column has missing values.
# It's like zooming in on the specific rows where we're missing some key info about our crew.

ranking_df[bool_series]

# Ayo, we're about to patch up those missing spots like a pro:

# We're using the .fillna() method to replace all missing values (NaN) in 'ranking_df' with 0.
# It's like fixing up our crew lineup, filling in the blanks with some placeholder values.

ranking_df.fillna(0)

# Ayo, we're about to patch up those missing spots like a pro:

# We're using the .fillna() method with the 'pad' method parameter to replace missing values (NaN) in 'ranking_df'.
# With 'pad', also known as 'ffill' (forward fill), we're filling in missing values with the last observed non-null value in the column.
# It's like sliding down the block and filling in the gaps with the last known info.

ranking_df.fillna(method='pad')

# We're bout to patch up those missing spots like a pro:

# We're using the .fillna() method with the 'bfill' method parameter to replace missing values (NaN) in 'ranking_df'.
# With 'bfill', also known as 'backfill' (backward fill), we're filling in missing values with the next observed non-null value in the column.
# It's like cruising through the block and filling in the gaps with the next piece of info.

ranking_df.fillna(method='bfill')

# Now it's time to level up and interpolate like a pro:

# We're using the .interpolate() method with the 'linear' method parameter to replace missing values (NaN) in 'ranking_df'.
# With 'linear' interpolation, we're filling in missing values by linearly interpolating between existing values in the column.
# It's like smoothly connecting the dots, filling in the blanks with a straight line.

ranking_df.interpolate(method='linear')

# Ayo, we're about to clean house and drop some rows like a boss:

# We're using the .dropna() method to drop rows from 'ranking_df' where any value is missing (NaN).
# It's like sweeping through the data and removing any rows that have missing or incomplete information.
# We're keeping it tight and only rolling with the complete data.

ranking_df.dropna()

# Ayo, we're about to clean house and drop some rows like a boss:

# We're using the .dropna() method with the 'how' parameter set to 'all'.
# This means we're only dropping rows from 'ranking_df' where all values are missing (NaN).
# It's like doing a thorough check and only removing rows that are completely empty.
# We're keeping it real and only getting rid of the rows that offer no valuable info.

ranking_df.dropna(how='all')

ranking_df.dropna(axis=1)
# Ayo, we're about to clean house and drop some rows like a boss:

# We're using the .dropna() method with the 'axis' parameter set to 0 (which is the default).
# This means we're dropping rows from 'ranking_df' where any value is missing (NaN).
# It's like doing a quick scan and getting rid of any rows that have missing or incomplete information.
# We're keeping it tight and only rolling with the complete data.

ranking_df.dropna(axis=0)
```

### Removing duplicates
```python
# Alright, fam, we're gearing up for some serious data expedition:

# First up, we're strapping on our climbing gear and summoning the almighty numpy library. 
# Just like belaying a partner up the rock face, numpy is our trusty rope, ready to support us through the toughest data climbs.

import numpy as np

# Next in line, we're setting our sights on the pandas library, our sturdy harness in this data adventure.
# Like navigating a challenging climbing route, pandas helps us maneuver through complex datasets with ease, ensuring we reach the summit of data excellence.

import pandas as pd

# And last but not least, we're reaching out for the Series and DataFrame classes from the pandas library, our essential tools in this data ascent.
# Just like selecting the perfect climbing shoes, these classes equip us with the precision and versatility needed to conquer any data challenge that comes our way.

from pandas import Series, DataFrame

# Alright, fam, we're about to tackle a new data climb:

# We're crafting a DataFrame called 'DF_obj' that represents a series of climbing routes.
# Each column represents a different aspect of the climb: 'difficulty' is the rating of the climb, 'route' is the type of climb, and 'name' is the name of the route.
# It's like mapping out a climbing guidebook, detailing each route's difficulty, type, and name.

DF_obj = DataFrame({'difficulty': [1, 1, 2, 2, 3, 3, 3],
                    'route': ['a', 'a', 'b', 'b', 'c', 'c', 'c'],
                    'name': ['A', 'A', 'B', 'B', 'C', 'C', 'C']})

# Now that we've crafted our climbing guidebook, let's check it out and see the routes we've mapped.
# Get ready to plan your next climbing adventure!

DF_obj

# Ayo, fam! We're about to do some route checking:

# We're using the .duplicated() method on 'DF_obj' to identify duplicate climbing routes.
# This method returns a boolean Series indicating whether each row is a duplicate (True) or not (False).
# It's like scanning our climbing guidebook and flagging any routes that appear more than once.

DF_obj.duplicated()
# Ayo, fam! We're about to clean up some route duplicates:

# We're using the .drop_duplicates() method on 'DF_obj' to remove duplicate climbing routes.
# This method returns a DataFrame with duplicate rows removed.
# It's like flipping through our climbing guidebook and ripping out any duplicate routes, ensuring each route is unique.

DF_obj.drop_duplicates()

# Ayo, fam! We're back with a new set of climbing routes:

# We're crafting a DataFrame called 'DF_obj' that represents a series of climbing routes.
# Each column represents a different aspect of the climb: 'difficulty' is the rating of the climb, 'route' is the type of climb, and 'name' is the name of the route.
# It's like updating our climbing guidebook with some fresh routes to explore.

DF_obj = DataFrame({'difficulty': [1, 1, 2, 2, 3, 3, 3],
                    'route': ['a', 'a', 'b', 'b', 'c', 'c', 'c'],
                    'name': ['A', 'A', 'B', 'B', 'C', 'D', 'C']})

# Now that we've crafted our updated climbing guidebook, let's check it out and see the routes we've mapped.
# Get ready to plan your next climbing adventure!

DF_obj

# Ayo, fam! We're about to clean up some route duplicates:

# We're using the .drop_duplicates() method on 'DF_obj' to remove duplicate climbing routes based on the 'name' column.
# This method returns a DataFrame with duplicate rows removed, considering only the 'name' column for comparison.
# It's like flipping through our climbing guidebook and ripping out any duplicate routes, ensuring each route name is unique.

DF_obj.drop_duplicates(['name'])
```

### Concatenating and transforming
```python
# Ayo, fam! We're gearing up for some serious data exploration, like navigating the vast expanse of our solar system:

# First up, we're summoning the almighty numpy library, ready to crunch numbers like a boss.
# Just like the gravitational pull of planets, numpy equips us with powerful tools for numerical computations.

import numpy as np

# Next in line, we're setting our sights on the pandas library, our trusty sidekick in this data adventure.
# Like orbiting planets around a star, pandas helps us manipulate and analyze datasets with ease, providing structure and organization to our data universe.

import pandas as pd

# And last but not least, we're cherry-picking the Series and DataFrame classes from the pandas library.
# These classes are like our essential satellites orbiting around a planet, allowing us to organize and structure our data for exploration and analysis.

from pandas import Series, DataFrame

### Concatenating data
# Ayo, fam! We're about to embark on a cosmic journey through the data universe:

# We're creating a DataFrame called 'DF_obj' that represents a matrix of data, akin to the celestial bodies in our solar system.
# Each element in the matrix is like a planet orbiting around the sun, with its own unique position and characteristics.

DF_obj = DataFrame(np.arange(36).reshape(6, 6))

# Now that we've laid out our cosmic matrix, let's take a closer look at the celestial bodies we've mapped.
# Get ready to explore the wonders of our data universe!

DF_obj

# Ayo, fam! We're back with another cosmic dataset:

# We're creating a DataFrame called 'DF_obj_2' that represents a matrix of data, similar to the celestial bodies in our solar system.
# Each element in the matrix is like a planet orbiting around the sun, with its own unique position and characteristics.

DF_obj_2 = DataFrame(np.arange(15).reshape(5, 3))

# Now that we've charted out our new cosmic matrix, let's take a closer look at the celestial bodies we've mapped.
# Get ready to explore the wonders of our data universe once again!

DF_obj_2

# Ayo, fam! We're about to merge two cosmic datasets into one epic cosmic exploration:

# We're using the pd.concat() function to concatenate (merge) two DataFrames, 'DF_obj' and 'DF_obj_2', along the columns (axis=1).
# It's like combining two star systems into one, merging their celestial bodies to explore a larger cosmic space.

pd.concat([DF_obj, DF_obj_2], axis=1)

# Ayo, fam! We're about to merge two cosmic datasets into one epic cosmic exploration:

# We're using the pd.concat() function to concatenate (merge) two DataFrames, 'DF_obj' and 'DF_obj_2', along the rows (axis=0).
# It's like combining two star systems into one, merging their celestial bodies to explore a larger cosmic space.

pd.concat([DF_obj, DF_obj_2], axis=0)

### Transforming data
#### Dropping data
# Ayo, fam! We're about to navigate through our cosmic dataset and drop some celestial bodies:

# We're using the .drop() method on 'DF_obj' to remove rows (celestial bodies) with indices 0 and 2 along the rows (axis=0).
# It's like navigating through our star map and eliminating specific planets to focus on a select group.

DF_obj.drop([0, 2], axis=0)

# Ayo, fam! We're about to navigate through our cosmic dataset and drop some celestial bodies:

# We're using the .drop() method on 'DF_obj' to remove columns (celestial bodies) with indices 0 and 2 along the columns (axis=1).
# It's like navigating through our star map and eliminating specific planets to focus on a select group.

DF_obj.drop([0, 2], axis=1)

#### Adding data
# Ayo, fam! We're about to add a new celestial body to our cosmic dataset:

# We're creating a Series called 'series_obj' that represents a new variable in our cosmic exploration.
# Each element in the Series is like a new celestial body orbiting around our cosmic dataset.
# We're giving this variable the name "added_variable" to distinguish it from other variables.

series_obj = Series(np.arange(6))
series_obj.name = "added_variable"

# Get ready to welcome our new celestial body to the cosmic data universe!

series_obj

# Ayo, fam! We're about to merge our cosmic dataset with a new celestial body:

# We're using the .join() method to merge the DataFrame 'DF_obj' with the Series 'series_obj', adding a new variable to our cosmic exploration.
# It's like combining two star systems, merging their celestial bodies to explore a larger cosmic space with new variables.

variable_added = DataFrame.join(DF_obj, series_obj)

# Get ready to explore the cosmic dataset with our new celestial body added!

variable_added

# Ayo, fam! We're about to double up our cosmic dataset for some epic cosmic exploration:

# We're using the pd.concat() function to concatenate (merge) the DataFrame 'variable_added' with itself, creating a new DataFrame 'added_datatable'.
# By setting ignore_index=False, we preserve the original index of the DataFrame.
# It's like duplicating our star map, creating a cosmic dataset with twice as many celestial bodies for exploration.

added_datatable = pd.concat([variable_added, variable_added], ignore_index=False)

# Get ready to dive deep into our expanded cosmic dataset and explore the wonders of the universe!

added_datatable

# Ayo, fam! We're about to double up our cosmic dataset for some epic cosmic exploration:

# We're using the pd.concat() function to concatenate (merge) the DataFrame 'variable_added' with itself, creating a new DataFrame 'added_datatable'.
# By setting ignore_index=True, we reset the index of the concatenated DataFrame, creating a continuous index from 0 to the total number of rows.
# It's like duplicating our star map, creating a cosmic dataset with twice as many celestial bodies for exploration, and renumbering the rows for clarity.

added_datatable = pd.concat([variable_added, variable_added], ignore_index=True)

# Get ready to dive deep into our expanded cosmic dataset and explore the wonders of the universe!

added_datatable

#### Sorting data
# Ayo, fam! We're about to sort our cosmic dataset based on a specific celestial body:

# We're using the .sort_values() method on 'DF_obj' to sort the DataFrame based on the values in the 5th column (celestial body), in descending order.
# It's like arranging our star map based on the characteristics of a specific celestial body, with the most massive ones at the top.

DF_sorted = DF_obj.sort_values(by=5, ascending=False)

# Get ready to explore our sorted cosmic dataset and discover the celestial bodies with the highest values!

DF_sorted
```

### Grouping and aggregation
```python
# Ayo, fam! We
# 're importing the almighty NumPy library, ready to crunch numbers like a boss.
import numpy as np

# Next up, we're summoning the powerful Pandas library, our trusty sidekick in this data adventure.
import pandas as pd

# And last but not least, we're cherry-picking the Series and DataFrame classes from the Pandas library.
# These classes are like our essential tools for organizing and structuring our data for exploration and analysis.
from pandas import Series, DataFrame
```

```python
# Ayo, fam! We're about to load some data into our cosmic data universe:

# We're setting the file path of our dataset to 'address'.
# It's like pinpointing the coordinates of a distant galaxy in the vast cosmic expanse.
address = '/workspaces/Python-for-Data-Science-and-Machine-Learning/mtcars.csv'

# Next, we're using the pd.read_csv() function to read the dataset located at the specified address.
# This function loads the dataset into a DataFrame called 'cars', ready for exploration and analysis.
# It's like capturing a cosmic image of a distant galaxy and bringing it back to our data observatory for study.
cars = pd.read_csv(address)

# Now that we've loaded our cosmic dataset, let's give the columns more meaningful names for easier analysis.
# We're renaming the columns to provide clearer descriptions of the data they represent.
cars.columns = ['car_name', 'mpg', 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb']

# Get ready to explore the first few rows of our cosmic dataset and uncover the mysteries within!
cars.head()
```

```python
# Ayo, fam! We're about to group our cosmic dataset based on the number of cylinders in each car:

# We're using the .groupby() method on the 'cars' DataFrame to group the data based on the 'cyl' column.
# This groups the cars into clusters based on the number of cylinders they have, creating cosmic car clusters!
cars_groups = cars.groupby(cars['cyl'])

# Now that we've grouped our cosmic cars, let's calculate the mean values for each numeric column within each group.
# We're using the .mean() method on the grouped DataFrame, specifying numeric_only=True to include only numeric columns in the calculation.
# It's like calculating the average properties of each cosmic car cluster, providing insights into their characteristics.
cars_groups.mean(numeric_only=True)
```